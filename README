# 💡 AI-Powered Oracle 23ai Query Assistant (POC)

This project is a proof-of-concept (POC) system that allows natural language questions to be converted into SQL or semantic search queries over data stored in Oracle Autonomous Database (ADB). It combines the power of Oracle 23ai's native vector search capabilities with a locally running LLM via Ollama to provide intelligent access to both structured and unstructured data.

---

+-------------------+         +-------------------------+
|                   |         |                         |
|   User / Client   +-------->+    FastAPI MCP Server   |
|                   |         |                         |
+--------+----------+         +-----------+-------------+
         |                                |
         |                                | Sends prompt + user query
         |                                v
         |                      +---------+---------+
         |                      |                   |
         |                      |   LLaMA 3.2 (LLM)  |
         |                      |   via Ollama       |
         |                      +---------+---------+
         |                                |
         |                Generates SQL or vector query
         |                                v
         |                      +---------+---------+
         |                      |                   |
         |                      | Oracle ADB (23ai) |
         |                      | - customers table |
         |                      | - documents table |
         |                      | - vector search   |
         |                      +---------+---------+
         |                                |
         |                Query results (structured or vector-based)
         |                                v
         +<-------------------------------+
               Cleaned and formatted result


## 🧱 System Components

### 🗄️ Oracle Autonomous Database (ADB)
- Stores two types of data:
  - **Structured data**: e.g., customer names, emails, and cities.
  - **Unstructured text**: e.g., complaints, occupations, languages.
- Supports vector search through **Oracle 23ai built-in embedding models**.
- Embeddings for documents are generated and stored inside the database itself.

### 🧠 Local LLM (LLaMA 3.2 via Ollama)
- Interprets user questions and generates structured responses in JSON format.
- Chooses between traditional SQL or semantic vector queries based on the nature of the question.
- Hosted locally using [Ollama](https://ollama.com).

### 🌐 FastAPI Server (MCP Server)
- Acts as the intermediary between the user interface, LLM, and the Oracle database.
- Handles:
  - Prompting the LLM with system instructions and user query
  - Receiving and executing the generated SQL or vector query
  - Returning formatted results to the user

---

## 🔁 Data Flow Overview

1. **User submits a natural language query**
   - Example: *“What languages do customers speak in Mumbai?”*

2. **FastAPI (MCP Server) sends the query and a predefined system prompt to LLaMA 3.2**
   - The system prompt helps the model understand the available tables and how to decide which one to use.

3. **LLM responds with a structured action**
   - Example: A SQL statement that uses vector search to retrieve relevant documents.

4. **The server runs the generated query against Oracle ADB**
   - If semantic search is used, Oracle generates the query embedding using its built-in model.

5. **Results are returned to the user**
   - CLOB fields and vector distances are parsed and cleaned before formatting.

---

## 🔧 Technologies Used

- **Oracle Autonomous Database (23ai)**
- **Oracle Vector Search**
- **LLaMA 3.2 (via Ollama)**
- **FastAPI**
- **Python + oracledb driver**

---

## 🚀 Highlights

- No external vector DB needed — all vector storage and querying happens inside Oracle ADB.
- Full use of Oracle's built-in `ALL_MINILM_L12_V2` model for both document and query embeddings.
- LLM smartly decides between structured SQL and semantic queries.
- Clean and fast FastAPI backend to support interactive usage and future UI integration.

---

## 📌 Status

✅ Embeddings inserted  
✅ LLM integrated via MCP server  
✅ Vector queries working  
✅ System prompt tuned for intelligent table usage  

Next steps might include:
- UI integration
- Confidence scoring
- Multi-modal document support (e.g., images, PDFs)



